{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the parallel model with CNN and Transformer\n",
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self, num_emotions=8):\n",
    "        super(ParallelModel, self).__init__()\n",
    "        \n",
    "        # Transformer block\n",
    "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1, 4], stride=[1, 4])\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=40, nhead=4, dim_feedforward=512, dropout=0.4, activation='relu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
    "\n",
    "        # CNN Block 1\n",
    "        self.conv2Dblock1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "\n",
    "        # CNN Block 2 (identical to Block 1)\n",
    "        self.conv2Dblock2 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "\n",
    "        # Final Linear layer\n",
    "        self.fc1_linear = nn.Linear(512 * 2 + 40, num_emotions)\n",
    "        self.softmax_out = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through CNN Block 1\n",
    "        conv2d_embedding1 = torch.flatten(self.conv2Dblock1(x), start_dim=1)\n",
    "        \n",
    "        # Forward pass through CNN Block 2\n",
    "        conv2d_embedding2 = torch.flatten(self.conv2Dblock2(x), start_dim=1)\n",
    "        \n",
    "        # Forward pass through Transformer Block\n",
    "        x_maxpool = self.transformer_maxpool(x)\n",
    "        x_maxpool_reduced = torch.squeeze(x_maxpool, 1)\n",
    "        x = x_maxpool_reduced.permute(2, 0, 1)\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "        transformer_embedding = torch.mean(transformer_output, dim=0)\n",
    "        \n",
    "        # Concatenate embeddings and pass through final FC layer\n",
    "        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2, transformer_embedding], dim=1)\n",
    "        output_logits = self.fc1_linear(complete_embedding)\n",
    "        output_softmax = self.softmax_out(output_logits)\n",
    "        return output_logits, output_softmax\n",
    "\n",
    "# Load model and weights\n",
    "model = ParallelModel(num_emotions=8)\n",
    "model.load_state_dict(torch.load('binaries/parallel_all_you_wantFINAL-429.pkl'))\n",
    "model.eval()\n",
    "\n",
    "# Test the model on your MFCC data\n",
    "def test_model(mfcc_data):\n",
    "    # Convert to tensor and add batch and channel dimensions\n",
    "    mfcc_tensor = torch.tensor(mfcc_data, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        _, predictions = model(mfcc_tensor)\n",
    "    \n",
    "    # Output predicted emotion\n",
    "    predicted_emotion = torch.argmax(predictions, dim=1).item()\n",
    "    print(f\"Predicted Emotion Class: {predicted_emotion}\")\n",
    "\n",
    "# Example usage with MFCC data\n",
    "# Assuming `mfcc_data` is your precomputed MFCC feature (2D array for one audio sample)\n",
    "# mfcc_data = ...\n",
    "# test_model(mfcc_data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
